{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_pysal_weights_moran_lisa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mar-roige/GIS-course/blob/master/6_pysal_weights_moran_lisaM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGsxVmn2-tsK",
        "colab_type": "text"
      },
      "source": [
        "#**Working with Pysal on Weights, Moran and Lisa: Spatial Statistical Analysis**#\n",
        "\n",
        "In this exercise we're going to get into some key spatial statistics. So far in this course we've mostly been visualising spatial distributions and patterns. Here we will run statistical tests to determine whether nor not a pattern or spatial structure exists, and to test what kind of pattern (dispersed vs. random vs. clustered) is present.\n",
        "\n",
        "This kind of analysis is what we would call **spatial statistical analysis**. This is different from ** deterministic analysis** -which is when you analyse things like which polygons intersect one another in a dataset, and which polygons entirely contain other polygons.  **Spatial statistical analysis** is a subset of **probabilistic analysis** - which is simply analysis of how likely it is that something happened. In this exercise we will start by working with vector-based data (points, lines and polygons). Then we will briefly look at working with raster datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqgoptAiXijR",
        "colab_type": "text"
      },
      "source": [
        "### Let's get started...####  \n",
        "<font color='orangered'> ~ déjà vu ~</font> \n",
        "* Make your own copy of this notebook;\n",
        "* Get your tools... you know install and import your libraries...\n",
        "* Remember to hit play or type 'Ctrl'+'Enter' to run the code in any cell (grey shaded cells in the page) to make things happen!\n",
        "\n",
        "Remember that broken thing two weeks ago? Yep, still broken... hence the long list of imported libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LlGBn4RQ1wL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_ImportUrLibraries\n",
        "\n",
        "# start by installing tools as usual\n",
        "!pip install geopandas\n",
        "!pip install descartes\n",
        "!pip install mapclassify\n",
        "!pip install pysal\n",
        "\n",
        "#@title\n",
        "!apt-get install -qq curl g++ make\n",
        "#@title\n",
        "!curl -L http://download.osgeo.org/libspatialindex/spatialindex-src-1.8.5.tar.gz | tar xz\n",
        "#@title\n",
        "import os\n",
        "os.chdir('spatialindex-src-1.8.5')\n",
        "#@title\n",
        "!./configure\n",
        "#@title\n",
        "!make\n",
        "#@title\n",
        "!make install\n",
        "#@title\n",
        "!pip install rtree\n",
        "#@title\n",
        "!ldconfig\n",
        "#Working through the example at http://toblerity.org/rtree/examples.html\n",
        "#@title\n",
        "from rtree import index\n",
        "from rtree.index import Rtree\n",
        "#@title\n",
        "p = index.Property()\n",
        "idx = index.Index(properties=p)\n",
        "idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbWJQH1CQrHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_ImportUrLibraries\n",
        "\n",
        "\n",
        "#and importing tools...\n",
        "import geopandas as gpd \n",
        "import requests \n",
        "import zipfile\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd \n",
        "import pysal as ps\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpZN9KC_ZxJ5",
        "colab_type": "text"
      },
      "source": [
        " **Learning a new language – decomposing the code** \n",
        "  <br>\n",
        "  in ##codecell_Spatial_Statistical_Analysis_ImportUrLibraries. These are what we call **prerequisites**. You know by now that they are basic tools so you can get started.\n",
        "* *Pandas* lets you manipulate your data. <font color='orangered'> ~ déjà vu ~</font>\n",
        "* *Geo-pandas* lets you manipulate your geographic data. <font color='orangered'> ~ déjà vu ~</font>\n",
        "* *requests* lets you access easily Hypertext Transfer Protocol or [HTTP](https://www.w3schools.com/tags/ref_httpmethods.asp) library ([doc.](https://realpython.com/python-requests/)).\n",
        "* *zipfile* lets you reate, read, write, append, and list a ZIP file.<font color='orangered'> ~ déjà vu ~</font>\n",
        "* *io* (input/open) lets you to access files and streams (a stream is sequence of data elements made available over time - processed 1 at a time)<font color='orangered'> ~ déjà vu ~</font>\n",
        "* *matplotlib* lets you plot in 2D which extensive plotting library for quality publication & *matplotlib.pyplot* provides a MATLAB-like way of plotting <font color='orangered'> ~ déjà vu ~</font>\n",
        "* *seaborn* lets you do statitistical data visualisation.<font color='orangered'> ~ déjà vu ~</font>\n",
        "* *numpy* lets you do statistics especially statistics using numpy.<font color='orangered'> ~ déjà vu ~</font>\n",
        "* *pysal* <font color='orangered'> ~ déjà vu ~</font> we have use it before in lab4_SpatialPatterns. This is the one I'd like you to spend a bit more time with...Why? because this library is used to conduct exploratory spatial data analysis. You can have a look at its [documentation](https://pysal.readthedocs.io/en/v1.11.0/users/introduction.html), a [video]((https://www.youtube.com/watch?v=FN1nH4Fkd_Y)) of one of its creator  and high level applications for spatial analysis (see image below).\n",
        "<div>\n",
        "<img src=\"https://sergerey.org/images/pysal.png\" width=\"800\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0UceQ6k5aby",
        "colab_type": "text"
      },
      "source": [
        "##**Preparing your Data**##\n",
        "\n",
        "You likely have realised by now that the most time consuming part of your work is preparing your data ... Data preparation is important, and your ability to use or re-use a dataset depends on how well it has been prepared.  When you are selecting and preparing your data, remember to think about your research aims and the questions you are trying to answer, and make sure your data is set up to respond to those aims and questions.\n",
        "\n",
        "The following scripts repeat what you have done in previous practicals, in particular Lab4_spatial_patterns_in_excavation where you complete the following basic steps:\n",
        "* import\n",
        "* read\n",
        "* select/extract\n",
        "* merge "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czrB1ZVBQwou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_ImportOurData\n",
        "\n",
        "# And now, as usual, get the data\n",
        "url = 'https://github.com/ropitz/spatialarchaeology/blob/master/gabii_spatial.zip?raw=true'\n",
        "local_path = 'temp/'\n",
        "\n",
        "print('Downloading shapefile...')\n",
        "r = requests.get(url)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "print(\"Done\")\n",
        "\n",
        "#here this is very much like what we are doing on your PC file system\n",
        "z.extractall(path=local_path) # extract to folder\n",
        "\n",
        "#here we want to download the shapefiles\n",
        "# so we sort them using sorted()\n",
        "#so we have a comprehensive list of all variables available in our dataset\n",
        "# print them\n",
        "filenames = [y for y in sorted(z.namelist()) for ending in ['dbf', 'prj', 'shp', 'shx'] if y.endswith(ending)] \n",
        "print(filenames)\n",
        "\n",
        "#here you know it ...\n",
        "#we use function read() to access the data from this notebook and work with it\n",
        "dbf, shp, shx = [filename for filename in filenames]\n",
        "gabii = gpd.read_file(local_path + 'gabii_SU_poly.shp')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoSWTv5nQ-XN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_Peek@OurData\n",
        "\n",
        "# As you've done before, print out some information on the data \n",
        "# To check the number of records in the file, so it has loaded in ok\n",
        "# And preview the data \n",
        "print(\"Shape of the dataframe: {}\".format(gabii.shape))\n",
        "print(\"Projection of dataframe: {}\".format(gabii.crs))\n",
        "gabii.tail() #last 5 records in dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8omTerlfvRB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_ObjectType&SU\n",
        "\n",
        "# As we've done before (returning to the Gabii finds data - see ###codecell_SpatialPatterns_WhichTypeOfSpecialFinds&fromWhere?) \n",
        "# Get the non-spatial special finds data\n",
        "# And they are archived per SU /Stratigraphical Unit\n",
        "sf_su = pd.read_csv(\"https://raw.githubusercontent.com/ropitz/gabii_experiments/master/spf_SU.csv\")\n",
        "sf_su.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msdQ_Jaq2S8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_FindsBecomesSpatial\n",
        "\n",
        "#Then let's combine our polygons representing context shape and location\n",
        "# with the special finds data\n",
        "# We can use known command  'merge()' for this (see especially explanation for this in #codecell_Webmaps&Distributions_MergingZeData and also ###codecell_SpatialPatterns_TextileToolsBecomesSpatial)\n",
        "\n",
        "gabii_textools = gabii.merge(sf_su, on='SU')\n",
        "gabii_textools.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toSx3S1kzsI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_WorkingWithPysal_DataPreparation\n",
        "\n",
        "#Let's pull all those find types out of the big list. \n",
        "#These commands should look familiar because you've done them before (###codecell_SpatialPatterns_SpecialFindsSelection)\n",
        "types = ['Loom Weight','Spool','Spindle Whorl']\n",
        "textile_tools = gabii_textools.loc[gabii_textools['SF_OBJECT_TYPE'].isin(types)]\n",
        "\n",
        "# Now let's count up how many of these tools appear in each context (SU).\n",
        "# This command will print out a list of the number of textile tools in each SU next to that SU number.\n",
        "textile_tool_counts = textile_tools.groupby('SU')['SF_OBJECT_TYPE'].value_counts().unstack().fillna(0)\n",
        "\n",
        "\n",
        "gts = gabii_textools.merge(textile_tool_counts, on='SU')\n",
        "gts_new = gts.drop_duplicates(subset=\"SU\")\n",
        "gts_new.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdEkesFQAKby",
        "colab_type": "text"
      },
      "source": [
        "##**Visualising your Data**##\n",
        "Now plot your data to visualise it with a focus on the Spool Distribution of Gabii excavations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gxMADn3dSge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_PlotZeData\n",
        "\n",
        "# Set up figure and axis\n",
        "f, ax = plt.subplots(1, figsize=(9, 9))\n",
        "# Plot SUs\n",
        "#gabii.plot(ax=ax, facecolor='0.85', linewidth=0)\n",
        "\n",
        "# Quantile choropleth of deaths at the street level\n",
        "gts_new.plot(column='Spool', scheme='fisher_jenks', ax=ax, \\\n",
        "        cmap='YlGn', legend=True, linewidth=3)\n",
        "# Plot pumps\n",
        "#xys = np.array([(pt.x, pt.y) for pt in pumps.geometry])\n",
        "#ax.scatter(xys[:, 0], xys[:, 1], marker='^', color='k', s=50)\n",
        "# Remove axis frame, also called graticule\n",
        "ax.set_axis_off()\n",
        "# Change background color of the figure\n",
        "f.set_facecolor('0.75')\n",
        "# Keep axes (your x and y) proportionate\n",
        "plt.axis('equal')\n",
        "# Title \n",
        "# f.suptitle()command allows you to add a centered title to the figure\n",
        "f.suptitle('Spool Distribution', size=30)\n",
        "# Draw\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4k2ydSEE8-c",
        "colab_type": "text"
      },
      "source": [
        "---------------------\n",
        "####**Visualisation**####\n",
        "\n",
        "They are different ways to visualise your variables. The best way to visually represent your data broadly depends on the type data present. Basic data types are:\n",
        "\n",
        "> data type | | | | |\n",
        "> ---|---|---|---|---\n",
        "> | Nominal|Qualitative ||\n",
        "> | Ordinal| Ranked |\n",
        "> |Numerical| Quantitative||\n",
        "\n",
        ">>>> <img src=\"http://article.sapub.org/image/10.5923.j.ajgis.20170601.02_002.gif\" width=\"500\"/> </div> \n",
        "\n",
        "In general, the image below provides a guide to good practice for visually representing different types of data as follows:\n",
        "\n",
        "* **qualitative data** :*Change of form, of pattern, of texture, of orientation, of hue*\n",
        "* **rank or ordering data** : *Variation in dimension, in colour or pattern strength lightness*\n",
        "* **quantitative data** :*Change in dimension, in lightness*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"http://article.sapub.org/image/10.5923.j.ajgis.20170601.02_003.gif\" width=\"700\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "---------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9lXmkgOAPio",
        "colab_type": "text"
      },
      "source": [
        "##**Spatial Statistical Analysis**##\n",
        "\n",
        "So far, you've (rapidly) repeated the steps you've done in a previous exercise to visualise a spatial pattern - this time of the spools discovered while excavating at Gabii. \n",
        "\n",
        "Now how do you statistically test if there is a pattern? Statistical tests are needed when it is not obvious from just looking at the distribution whether or not a pattern exists. We can start with some of the more basic tests: **Moran's** and **local Moran's**, which are tests for spatial autocorrelation. Moran's I statistic (1948, 1950) is one of the classic (as well as one of the most common) ways of measuring the degree of spatial autocorrelation in data. \n",
        "\n",
        "**But how does this work? ** You have seen that objects in space are rarely randomly distributed. In fact, they usually have some degree of patchiness (i.e., they are spatially clustered) which can produce a variety of distinct spatial patterns. <br> These patterns can be quantified according to the degree of similarity between the attributes values of adjacent spatial objects (polygons, lines, points, raster cells). <br>Start by testing  if  nearby  objects  tend  to  have  similar  attributes  (Figure  below(a))  than expected from a random distribution (Figure 6.1(b)). \n",
        "      \n",
        "\n",
        "\n",
        "<div> <img src=\"https://raw.githubusercontent.com/Francoz-Charlotte/Spatial_teaching_CFediting/master/Untitled.png\" width=\"700\"/> </div>\n",
        "\n",
        "The degree to which similar values cluster together spatially is the degree of autocorrelation. This value is reported as **Moran's I**.\n",
        "\n",
        "Spatial  statistics  assume  that,  within the  study  area,  the  parameters  of  the  function  defining  the  underlying  process,  such  as  the  mean  and  the variance, are constant regardless of the distance and direction between the sampling locations. This property of the random function is known as *spatial stationarity*. \n",
        "\n",
        "The goal of spatial statistics is **to test the null hypothesis of absence of ‘spatial pattern’**. For each spatial statistic a ‘spatial pattern’ is either spatial aggregation or segregation (Ripley's K; joint count statistics) or spatial autocorrelation (Moran's I and Geary's  c).  The  null  hypothesis  implies  that attributes of neighbouring points are independent and do not  affect  one another.  **The  alternatives**  are  that there is clustering which we refer to as positive spatial autocorrelation or repulsion which we refer to as negative spatial auotocorrelation.\n",
        "\n",
        "(*Fortin, M. & Dale, M. (2009). Spatial autocorrelation. In Fotheringham, A. S., & Rogerson, P. A. The SAGE handbook of spatial analysis (pp. 89-103). London: SAGE Publications,*). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO-8JxfLWFlZ",
        "colab_type": "text"
      },
      "source": [
        "###**Moran's I test**###\n",
        "\n",
        "In a nutshell, the Moran’s I statistic provides a correlation coefficient (a measurement of similarity) for the relationship between a measured value and its surrounding measured values. \n",
        "\n",
        "To understand the meaning of this measurement of similarity, the Moran's I value, you need to understand how \"statistical significance\" works. Basically, a measurement is considered \"statistically significant\" if it very different from what you would get by chance. A spatial pattern is \"statistically significant' if it is very different from a random spatial pattern.  \n",
        "\n",
        "In summary:\n",
        "* null hypothesis = the data is randomly disbursed.\n",
        "* alternative hypothesis = the data is more spatially clustered than you would expect by chance alone. \n",
        "\n",
        "Two possible scenarios are:\n",
        "* A positive z-value: data is spatially clustered in some way, but, not with very high cluster or with very low cluster values.\n",
        "* A negative z-value: data is dispersed in some way. For example, high values may be repelling high values or negative values may be repelling negative values. This suggests the presence of spatial outliers and spatial heterogeneity (like image -c- above).\n",
        "\n",
        "You can read about [Moran's](https://mgimond.github.io/Spatial/spatial-autocorrelation.html). If you prefer a video, this one  [this](https://www.youtube.com/watch?v=_J_bmWmOF3I) one is very clear on how to interpret the resulst of a Moran's test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozpmT-c_iALL",
        "colab_type": "text"
      },
      "source": [
        "####**To start your Moran's I test**###\n",
        "\n",
        "As you may have suspected, we need to do something first before running the Moran's I. And, as said above, the emphasis of the Moran's I spatial analysis is on the role of the weights. So, one of the vital steps of spatial autocorrelation modelling is to construct a spatial weights matrix. The spatial weights can be based on:\n",
        "* Contiguity [Pysal-notes](https://pysal.readthedocs.io/en/v1.11.0/users/tutorials/weights.html#contiguity-based-weights)\n",
        "* Distance [Pysal-notes](https://pysal.readthedocs.io/en/v1.11.0/users/tutorials/weights.html#distance-based-weights)\n",
        "* Distance band [Pysal-notes](https://pysal.readthedocs.io/en/v1.11.0/users/tutorials/weights.html#distance-band-weights)\n",
        "* Kernel [Pysal-notes](https://pysal.readthedocs.io/en/v1.11.0/users/tutorials/weights.html#kernel-weights)\n",
        "* K-nearest neighbours [Pysal-notes](https://pysal.readthedocs.io/en/v1.11.0/users/tutorials/weights.html#k-nearest-neighbor-weights)\n",
        "which can be illustrated as such:\n",
        "\n",
        "<div><img src=\"https://cdn-images-1.medium.com/max/800/0*QmLAPLYUDcpJYwvo.png\" width=\"600\"/></div> *where 0= no weights and 1\n",
        "\n",
        "\n",
        "As we've discussed in class, designing a good weights matrix is part of the interpretive process. It's an important step because if we fail to design the weights matrix well, the resulting of spatial analysis will likely not produce results that are meaningful. It is important, therefore, to understand the **kinds of spatial relationships** listed above, and to use the one that best represents the kind of spatial relationship presesnt in your data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD_Oz57v8L6S",
        "colab_type": "text"
      },
      "source": [
        "#####**Let's define the weights**#####\n",
        "\n",
        " Let's create some weights for a real dataset that you know: finds from the Gabii excavations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DOM4mY88XQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_MoranStatistics_defineweights\n",
        "\n",
        "# To start your Moran's statistical test, you need to create weights that define how strongly you think things near to one another influence one another.\n",
        "# see the types of weights available to you by looking in pysals help file\n",
        "help(ps.lib.weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSwfX24T8Jwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_MoranStatistics_KNN_weights\n",
        "\n",
        "#create some weights. I've gone with KNN weights. Read about this in the pysal documentation linked above...\n",
        "#we want to see what is happening with the Spools \n",
        "\n",
        "#let's first subselect them and create a gts_spool\n",
        "gts_spool = gts_new[['SU','Spool']]\n",
        "\n",
        "#let's have the nearest neighbours weights \n",
        "#we choose a k-distance of 5\n",
        "gts_spool_weights = ps.lib.weights.KNN(gts_spool,5)\n",
        "#here we Ignore the warnings because\n",
        "#we know that not all the SU areas connect up physically"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92a8jWAjDUrr",
        "colab_type": "text"
      },
      "source": [
        "#####**Let's add the knn-weights as an attribute to your data**#####\n",
        "\n",
        "\n",
        "Here we create a matrix, or a grid of values,  which contains the weights assigned to each spatial entity: <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Matrix.svg/1200px-Matrix.svg.png\" width=\"400\"/>  <img src=\"https://petewarden.files.wordpress.com/2015/04/fcgemm_corrected.png?w=768&h=253\" width=\"400\"/>.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5M2Z7HkAIah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_MoranStatistics_DefineMatrix\n",
        "\n",
        "# Rename IDs to match those in the `segIdStr` column\n",
        "gts_spool_weights.remap_ids(gts_spool.index)\n",
        "\n",
        "# Row standardise the matrix\n",
        "gts_spool_weights.transform = 'R'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-ByLSt8fg2M",
        "colab_type": "text"
      },
      "source": [
        " **Learning a new language – decomposing the code** \n",
        "  <br>\n",
        "  in *##codecell_Spatial_Statistical_Analysis_MoranStatistics_DefineMatrix*, to keep things aligned, you use <font color='magenta'> .remap_ids() </font>  to rename the IDs of the matrix to match those in the table.\n",
        "\n",
        "Often there is a need to apply a transformation <font color='magenta'> .transform = 'R' </font> to the spatial weights, such as in the case of row standardisation. You can use .transform = 'b' for binary and \n",
        ".transform = 'v' for variance stabilising [doc](https://pysal.readthedocs.io/en/v1.11.0/users/tutorials/weights.html#weight-transformations). \n",
        "\n",
        "**~ Behind the scenes ~** the transform property is updating all other characteristics of the spatial weights that are a function of the values and these standardisation operations, freeing the user from having to keep these other attributes updated. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvoI0eAmgy9H",
        "colab_type": "text"
      },
      "source": [
        "#####**Let's add Spatial lag**#####\n",
        "Now you have the data and the spatial weights matrix ready.\n",
        "\n",
        "Let's start by computing the spatial lag of the spool distribution. The spatial lag is the product (multiplication) of the spatial weights matrix and a given variable. If gts_spool_weight  is row-standardised, the result is the average value of the variable in the neighborhood of each observation.\n",
        "\n",
        "Multiplication of two matrices looks like this:\n",
        "<div><img src=\"https://hadrienj.github.io/assets/images/2.2/dot-product.png\" width=\"500\"/></div>\n",
        "\n",
        "\n",
        "We can calculate the spatial lag for the variable 'Spool' and store it directly in the main table with <font color='magenta'> .lag_spatial() </font>:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTPTQRiICR7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_MoranStatistics_SpatialLag\n",
        "\n",
        "#add the weights you've created to the attribute table\n",
        "gts_spool['gts_spool_weights'] = ps.lib.weights.lag_spatial(gts_spool_weights, gts_spool['Spool'])\n",
        "gts_spool.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK-Sx1HwDuwd",
        "colab_type": "text"
      },
      "source": [
        "#####**Standardisation against biaised dataset**#####\n",
        "\n",
        "Whenever there is a risk that the distribution of your features (see decomposing the code of *#codecell_RasterLandscape_BandsDataType* as a reminder) is potentially biased due to sampling design or an imposed aggregation scheme, it is important to row standardize the data. \n",
        "\n",
        "Read about [standardisation](http://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-statistics-toolbox/modeling-spatial-relationships.htm#GUID-DB9C20A7-51DB-4704-A0D7-1D4EA22C23A7) in spatial modelling.\n",
        "\n",
        "\n",
        "So now we need to remove potentially biased distribution in our dataset by standardising the counts and standardising the means."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8uUaea3DHAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_MoranStatistics_Standardisation&SpatialLag\n",
        "\n",
        "# standardise the counts of the number of spools in each context and the weights\n",
        "#we are applying some math here\n",
        "gts_spool['spool_std'] = (gts_spool['Spool'] - gts_spool['Spool'].mean()) / gts_spool['Spool'].std()\n",
        "gts_spool['w_spool_std'] = ps.lib.weights.lag_spatial(gts_spool_weights, gts_spool['spool_std'])\n",
        "gts_spool.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk5JjmcJr_LN",
        "colab_type": "text"
      },
      "source": [
        "**Learning a new language – decomposing the code** \n",
        "  <br>\n",
        "  in *##codecell_Spatial_Statistical_Analysis_MoranStatisticstandardisation&SpatialLag*, to make sure our varaibles distribution is not biaised, we apply some basic algebra:\n",
        "\n",
        "<div><img src=\"https://raw.githubusercontent.com/Francoz-Charlotte/Spatial_teaching_CFediting/master/Lab6_MoransI_stand%26SpatialLag_.png\" width=\"1200\"/></div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GDFkBUZElPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_MoransI_ImportUrLibraries\n",
        "\n",
        "\n",
        "#get some more tools for the Moran test\n",
        "from pysal.explore.esda.moran import Moran"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAMa3J_IEA6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_RUNMoransI\n",
        "\n",
        "# Run the Moran test\n",
        "mi = Moran(gts_spool['Spool'], gts_spool_weights)\n",
        "mi.I\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtGalQ97D4FE",
        "colab_type": "text"
      },
      "source": [
        "#####**Interpreting your Moran's I**#####\n",
        "*##codecell_Spatial_Statistical_Analysis__RUNMoransI*  mean? \n",
        "* Read how to [interpret the results](https://www.statisticshowto.datasciencecentral.com/morans-i/).\n",
        "* Are your spools actually clustered?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRL98mVV4DIo",
        "colab_type": "text"
      },
      "source": [
        "#####**Let's plot your Moran's I**#####\n",
        "\n",
        " Now let's plot the results.\n",
        "\n",
        "The cluster/outlier type (COType) field distinguishes between a statistically significant cluster of:\n",
        "* high values (HH), \n",
        "* cluster of low values (LL), \n",
        "* outlier in which a high value is surrounded primarily by low values (HL)\n",
        "* outlier in which a low value is surrounded primarily by high values (LH). \n",
        "\n",
        "\n",
        "Statistical significance is set at the 95 percent confidence level (really quite confident, we could say). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ybhc4zC4Ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_PLOTMoransI\n",
        "\n",
        "#Here we are using similar matplotlib commands than in Lab3_StreetOrientation & Lab5_RasterLandscape \n",
        "\n",
        "# Setup the figure and axis\n",
        "f, ax = plt.subplots(1, figsize=(9, 9))\n",
        "\n",
        "# Plot values \n",
        "#command sns.regplot()allows you to plot the data/results and a linear regression model fit \n",
        "#such as our results of Moran's I \n",
        "#\n",
        "sns.regplot(x='spool_std', y='w_spool_std', data=gts_spool)\n",
        "\n",
        "# Add vertical and horizontal lines\n",
        "plt.axvline(0, c='k', alpha=0.5)\n",
        "plt.axhline(0, c='k', alpha=0.5)\n",
        "ax.set_xlim(-2, 7)\n",
        "ax.set_ylim(-2.5, 2.5)\n",
        "\n",
        "#add text within each quandrant\n",
        "plt.text(3, 1.5, \"HH\", fontsize=25)\n",
        "plt.text(3, -1.5, \"HL\", fontsize=25)\n",
        "plt.text(-1, 1.5, \"LH\", fontsize=25)\n",
        "plt.text(-1, -1.5, \"LL\", fontsize=25)\n",
        "\n",
        "# Display\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d3N1txj6iAO",
        "colab_type": "text"
      },
      "source": [
        "**Learning a new language – Interpreting the code results**\n",
        "\n",
        "What's the figure generated from *##codecell_Spatial_Statistical_Analysis__PLOTMoransI*  mean? \n",
        "\n",
        "In order to guide the interpretation of the plot, a linear fit is also included in the plot, together with confidence intervals. This line represents **the best linear fit** to the scatter plot or, in other words, what is the best way to represent the relationship between the two variables as a straight line. Because the line comes from a regression, we can also include a measure of the uncertainty about the fit in the form of confidence intervals (the shaded blue area around the line).\n",
        "\n",
        "The plot displays a positive relationship between both variables. This is associated with the presence of positive spatial autocorrelation: similar values tend to be located close to each other. If we had to summarise the main pattern of the data in terms of how clustered similar values are, the best way would be to say they are positively correlated and, hence, clustered over space.\n",
        "\n",
        "At the core of this is a classification of the observations in a dataset into four groups derived from the Moran Plot: high values surrounded by high values (HH), low values nearby other low values (LL), high values among low values (HL), and viceversa (LH). Each of these groups are typically called \"quadrants\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-wxTR_aElbr",
        "colab_type": "text"
      },
      "source": [
        "###**Moran's I to Moran's local: LISA**###\n",
        "\n",
        "Actually, as you may have realised, Moran's I can tell you about the complete spatial pattern so it tells you about the clustering BUT it tells us nothing about where the clusters might be !  It only tells you that the pattern is more clustered than it would be if it would be under spatial randomness. \n",
        "\n",
        "So why do we need Moran's I then? well you need to test the significance of your spatial analysis, i.e. that your regression is not violating your assumptions... so Moran's is a global test.\n",
        "\n",
        "And, now, what do we do to get test the location of spatial clusters? We need a local statistic...To test this, we use the local variant of the Moran's test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ywdvVrCtmaq",
        "colab_type": "text"
      },
      "source": [
        "####**But are there local patterns inside the global one?**####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neClfc59E4m7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_ImportUrLibraries\n",
        "\n",
        "# get the tools for the local test\n",
        "from pysal.explore.esda.moran import Moran_Local"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpwGWNguE9lS",
        "colab_type": "text"
      },
      "source": [
        "####**Using your Moran's local**####\n",
        "\n",
        "The local test breaks the global pattern down to **test for the presence of local clusters**. You can check at each SU whether or not it is likely (in a statistical significance sense) for it to participate in a local cluster.\n",
        "\n",
        "*How does this work?*  Local measures consider each single observation in a dataset and operate on them (as oposed to on the overall data) as global measures do. \n",
        "\n",
        "<font color='magenta'> **LISAs (Local Indicators of Spatial Association)** </font>  are widely used in many fields to identify clusters of values in space. They are a very useful tool that can quickly return areas in which values are concentrated and provide suggestive evidence about the processes that might be at work. Therefore, they are a key part of the exploratory toolset. In Python, we can calculate LISAs in a very streamlined way thanks to PySAL:\n",
        "\n",
        "* **A positive value** for I indicates that a feature has neighboring features with similarly high or low attribute values; this feature is part of a cluster.\n",
        "* **A negative value** for I indicates that a feature has neighboring features with dissimilar values; this feature is an outlier. \n",
        "* In either instance, the **p-value** for the feature must be small enough for the cluster or outlier to be considered statistically significant. \n",
        "* **Note** that the local Moran's I index (I) is a relative measure and can only be interpreted within the context of its computed z-score or p-value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3-URUxzEx3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_MoransLocal\n",
        "\n",
        "# run the local test\n",
        "lisa = Moran_Local(gts_spool['Spool'].values, gts_spool_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0dukgj__PNt",
        "colab_type": "text"
      },
      "source": [
        "####**Applying your Moran's local**####\n",
        "\n",
        "All we need to pass is the <font color='magenta'> .values </font> variable of interest <font color='green'> \"gts_spool['Spool']\" </font>  and the spatial weights <font color='green'> \"gts_spool_weights\" </font>  that describes the neighborhood relations between the different observation that make up the dataset.\n",
        "\n",
        "Looking at the numerical result of LISAs is not always the most useful way to exploit all the information they can provide. Remember that we are calculating a statistic for every sigle observation in the data so, if we have many of them, it will be difficult to extract any meaningful pattern. Instead, what is typically done is to create a **cluster map** that extracts the significant observations (those that are highly unlikely to have come from pure chance) and plots them with a specific color depending on their quadrant category.\n",
        "\n",
        "All of the information needed is contained in the <font color='magenta'> lisa </font> object you have just created in *##codecell_Spatial_Statistical_Analysis_MoransLocal*. \n",
        "\n",
        "\n",
        "It is convenient to pull them out and insert them in the main data frame that we will call <font color='green'> \"gts_spool\" </font> as demonstrated in *##codecell_Spatial_Statistical_Analysis_MoransLocal_Significance*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JnBisEIFqhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_MoransLocal_Significance\n",
        "\n",
        "# Break observations into significant or not\n",
        "# using .o_sim < we will set it at 95% confidence interval  \n",
        "# so significant values are all the ones which are less than 5% \n",
        "gts_spool['significant'] = lisa.p_sim < 0.05\n",
        "\n",
        "# By using .q command\n",
        "# you can store the quadrant each observation belongs to:\n",
        "# - the high-high, high-low, low-high, low-low \n",
        "# just as before (in ##codecell_Spatial_Statistical_Analysis_PLOTMoransI) are quads 1-4 \n",
        "gts_spool['quadrant'] = lisa.q\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5S17HgEB1Lr",
        "colab_type": "text"
      },
      "source": [
        "**Learning a new language – Interpreting the code results**\n",
        "\n",
        " First, the <font color='magenta'> 'significance'</font> column. As with global Moran's I, PySAL is automatically computing a p-value for each LISA. Because not every observation represents a statistically significant one, we want to identify those with a p-value small enough that rules out the possibility of obtaining a similar situation from pure chance. Since we have been working with a 95% confidence interval, we select 5% using <font color='magenta'> p.sim< </font> as the threshold for statistical significance (just like with global Moran's I). \n",
        "\n",
        "To identify these values, we create in *##codecell_Spatial_Statistical_Analysis_MoransLocal_SignificanceResults* below a variable, significant, that contains :\n",
        " * **True** if the p-value of the observation is satisfies the condition, and \n",
        " * **False** otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcB0rHtvB67c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_MoransLocal_SignificancereadingUrResults\n",
        "\n",
        "gts_spool['significant'][:20]\n",
        "# true means it is in a cluster, false means it is not"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goVU6rWyGDCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_MoransLocal_SignificancereadingUrResults\n",
        "\n",
        "# You can read out the calculated p values for each \n",
        "lisa.p_sim[:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-resv_GpGy_Z",
        "colab_type": "text"
      },
      "source": [
        "####**Bringing your Moran's local to your dataset**####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lksLfhAyH76s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_AddingUrMoransLocal2Gabiidataset\n",
        "\n",
        "#add this info back onto the spatial data\n",
        "gabii_spool_lisa = gabii.merge(gts_spool, on='SU')\n",
        "gabii_spool_lisa.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cglqoAI4JAyC",
        "colab_type": "text"
      },
      "source": [
        "#####**Let's plot your Moran's Local**#########\n",
        "\n",
        "Now we have these elements, the <font color='magenta'> signifigance </font>  and <font color='magenta'> quadrant </font>, we can make a map showing which quadrant each SU belongs to, essentially a display of where the local clusters are located.\n",
        "\n",
        "**Parameters** are for:\n",
        "* High-high = 4\n",
        "* Low-low = 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP20J-TWGMsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_MappingGabiiMoransLocal\n",
        "\n",
        "# Setup the figure and axis\n",
        "f, ax = plt.subplots(1, figsize=(9, 9))\n",
        "\n",
        "# Plot baseline su poly\n",
        "gabii_spool_lisa.plot(column='quadrant',  ax=ax, \\\n",
        "        cmap='Accent', legend=True, linewidth=3)\n",
        "\n",
        "ax.set_axis_off()\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcivQCnTJ4-r",
        "colab_type": "text"
      },
      "source": [
        "#####**Interpreting your Moran's Local**#########\n",
        "\n",
        "How would you interpret the results of this analysis?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auhfktQSqVAU",
        "colab_type": "text"
      },
      "source": [
        "###**From Vector to Raster**###\n",
        "\n",
        "You can run similar spatial statistics on raster data to understand trends and distributions of data. \n",
        "\n",
        "It is possible to convert point vector data to raster data via the process of interpolation. \n",
        "\n",
        "<font color='orangered'> ~ déjà vu ~</font> We have talked about this before:\n",
        "* in *Lab2_webmaps&distributions* when presenting type of data distribution and example of applications; and,\n",
        "* in *Lab5_raster_landscape* when working with DEMs, you remember rasters with an elevation attribute which was interpolated to create a surface.\n",
        "\n",
        "<div><img src=\"https://github.com/Francoz-Charlotte/Spatial_teaching_CFediting/blob/master/Lab6_Interpolation_.png?raw=1\" width=\"400\"/></div>\n",
        "\n",
        "There are many ways to interpolate data, which you can read about on this [site](https://www.neonscience.org/spatial-interpolation-basics). \n",
        "\n",
        "*Examples of Applications*\n",
        "<img src=\"https://github.com/Francoz-Charlotte/Spatial_teaching_CFediting/blob/master/Examples%20of%20applications.png?raw=1\" width=\"700\"/> </div> \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W9_wwCjrNwM",
        "colab_type": "text"
      },
      "source": [
        "####**Whitebox is an open source GIS**####\n",
        "<font color='magenta'> **Whitebox** </font> is full of hundreds (412 for version 1.0.2!) of useful spatial tools. \n",
        "\n",
        "You can download it and run it standalone (without installing!) or you can call its functions from the jupyter notebook. \n",
        "\n",
        "It will be well worth checking out for your own independent projects. Read about [Whitebox Tools](https://github.com/jblindsay/whitebox-tools)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h63q0-mr7LNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_InstallUrLibraries\n",
        "\n",
        "# we're going to use the whitebox tools to interpolate our data and imageio to view it\n",
        "!pip install whitebox # acts as a spatial toolbox\n",
        "!pip install imageio \n",
        "!pip install imageio tifffile # stores numpy arrays in TIFF format files & read image&metadata from TIFFs "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpg4bRw07N3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_ImportUrLibraries\n",
        "\n",
        "# import your tools and print the version to check it worked\n",
        "\n",
        "import pkg_resources # provides runtime facilities for finding, introspecting, activating and using installed Python distributions\n",
        "import whitebox #advanced geospatial data analysis platform. Well you need its toolbox for that (see 3 codeline down)!\n",
        "import imageio # provides an easy interface to read and write a wide range of image data, including animated images, volumetric data, and scientific formats\n",
        "import IPython # for displaying html outputs\n",
        "import os\n",
        "\n",
        "wbt = whitebox.WhiteboxTools()\n",
        "print(wbt.version())\n",
        "print(wbt.help())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ep29_-C7hOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_CheckUrNewSpatialToolbox\n",
        "\n",
        "# when there are lots of tools, it's useful to call their help info to find out what the parameters are\n",
        "print(wbt.tool_help(\"IdwInterpolation\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgiUOkxApm8Z",
        "colab_type": "text"
      },
      "source": [
        "#####**Let's interpolate**#####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mr8zlMvlMUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_PrepareUrData\n",
        "\n",
        "# To interpolate, as you may have noticed in your reading, we need points rather than polygons. \n",
        "# To have points, we get the center of each polygon, called centroids. \n",
        "\n",
        "# In these lines we essentially strip out the geometry information when we get the center of each polygon. \n",
        "spool_centroids = gabii_spool_lisa.geometry.centroid\n",
        "spoolgdf = gpd.GeoDataFrame(geometry=gpd.GeoSeries(spool_centroids))\n",
        "\n",
        "#Then we 'merge' to glue the attributes we are interested in interpolating back on to our data.\n",
        "spool_centroids_attr = spoolgdf.merge(gabii_spool_lisa.spool_std,right_index=True,left_index=True)\n",
        "\n",
        "#Let's have a sneaky peek at the dataframe\n",
        "spool_centroids_attr.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPEPVLUm2xgM",
        "colab_type": "text"
      },
      "source": [
        "#####**Let's get our working directory sorted**#####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt38Uahqybi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_SetUrWorkingDirectory\n",
        "\n",
        "# to work in whitebox effectively, we put this data out to a temporary shapefile\n",
        "# we also set a data directory where we can put this file\n",
        "data_dir = '/usr/local/lib/python3.6/dist-packages/whitebox/testdata/'\n",
        "wbt.set_working_dir('data_dir')\n",
        "print(data_dir)\n",
        "\n",
        "#wbt.verbose command allows you to decide if you want output messages such as warnings, progress updates and other notifications\n",
        "wbt.verbose = True\n",
        "\n",
        "#Let's place our shapefile in a file \n",
        "gabii_spool_shapes = spool_centroids_attr.to_file(\"/usr/local/lib/python3.6/dist-packages/whitebox/testdata/gabii_spool.shp\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9oPY80wGWTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_InterpolateRaster\n",
        "\n",
        "# now we interpolate our centriods using the standardised spool weights and show the result\n",
        "# note you can play around with the interpolation parameters to have different kernel sizes and cell sizes...\n",
        "\n",
        "wbt.idw_interpolation(\n",
        "    i=\"/usr/local/lib/python3.6/dist-packages/whitebox/testdata/gabii_spool.shp\", \n",
        "    field=\"spool_std\",\n",
        "    output=\"/usr/local/lib/python3.6/dist-packages/whitebox/testdata/gabii_spool.tif\", \n",
        "    use_z=False, \n",
        "    weight=2.0, \n",
        "    radius=5, \n",
        "    min_points=3, \n",
        "    cell_size=1, \n",
        "    base=None\n",
        ")\n",
        "\n",
        "raster = imageio.imread(\"/usr/local/lib/python3.6/dist-packages/whitebox/testdata/gabii_spool.tif\")\n",
        "plt.imshow(raster)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yoIwIzgJm_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#List data under the data folder\n",
        "print(os.listdir('testdata'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLkWKM_rvit-",
        "colab_type": "text"
      },
      "source": [
        "#####**Let's get the spatial statistical Analysis running**##\n",
        "\n",
        "Now you can calculate statistics on this dataset, for example using **rasterio** like we did last week, or using **whitebox's tools**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un1fJHefODET",
        "colab_type": "text"
      },
      "source": [
        "######**Let's check distribution**########\n",
        "For example, you can check to see if the data forms a normal distribution. This lets you know what kinds of statistical tests will be valid. As we discussed last week, often archaeological distributions are not normal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnUQMaqSy_Kw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_RasterSpatialStats_normality\n",
        "\n",
        "wbt.ks_test_for_normality(\n",
        "    \"/usr/local/lib/python3.6/dist-packages/whitebox/testdata/gabii_spool.tif\", \n",
        "    \"/usr/local/lib/python3.6/dist-packages/whitebox/testdata/normal.html\", \n",
        "    num_samples=None\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w41lhagV1P_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_RasterSpatialStats_ViewNormality\n",
        "\n",
        "# view your results\n",
        "IPython.display.HTML(filename=\"/usr/local/lib/python3.6/dist-packages/whitebox/testdata/normal.html\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thKxqY3F2AvM",
        "colab_type": "text"
      },
      "source": [
        "######**Let's check for autocorrelation**########\n",
        "\n",
        "\n",
        "You can also run a similar autocorrelation test to the LISA one we ran above with the vector data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmqoKZKP2GDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_RasterSpatialStats_Autocorrelation\n",
        "\n",
        "wbt.image_autocorrelation(\n",
        "    \" /usr/local/lib/python3.6/dist-packages/whitebox/testdata/gabii_spool.tif\", \n",
        "    \"/usr/local/lib/python3.6/dist-packages/whitebox/testdata/gabii_autocorr.html\", \n",
        "    contiguity=\"Rook\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw95v0Qw2hU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_Spatial_Statistical_Analysis_RasterSpatialStats_ViewAutocorrelation\n",
        "\n",
        "# view your results\n",
        "IPython.display.HTML(filename='/usr/local/lib/python3.6/dist-packages/whitebox/testdata/gabii_autocorr.html')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4x4PpWcJ76O",
        "colab_type": "text"
      },
      "source": [
        "This exercise ends here. Hopefully you've learned that there are statistical tests for spatial patterns and that these let us go beyond 'just visualising' to look for patterns. These tests can be run on either vector or raster data.\n",
        "\n",
        "Clearly there's a lot to learn and explore in the world of spatial stats..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U2G_IqKe2Wl",
        "colab_type": "text"
      },
      "source": [
        "#**LexiCode**#\n",
        "To re-use the codes - you will need to first load their respective libraries.  So far, you have used ...:\n",
        "\n",
        "<br>\n",
        "\n",
        ">libraries \t|\t \t|\t \t|\n",
        ">--- \t|\t--- \t|\t --- \t|\n",
        ">[folium](https://python-visualization.github.io/folium/) \t|\t [numpy](https://numpy.org/)  \t|\t [rasterio](https://rasterio.readthedocs.io/en/stable/quickstart.html)\t|\n",
        ">[branca](https://pypi.org/project/branca/)\t|\t [rtree](https://pypi.org/project/Rtree/) \t|\t [richdem](https://richdem.readthedocs.io/en/latest/)\t|\n",
        ">[pandas](https://pandas.pydata.org/)\t|\t [osmnx](https://osmnx.readthedocs.io/en/stable/)\t|\t [elevation](https://pypi.org/project/elevation/)\t|\n",
        ">[geopandas](http://geopandas.org/)\t|\t [requests](https://realpython.com/python-requests/) \t|\t [zipfile](https://docs.python.org/3/library/zipfile.html) \t|\n",
        ">[seaborn](https://seaborn.pydata.org/index.html) \t|\t [fiona](https://pypi.org/project/Fiona/)\t|\t [io](https://docs.python.org/3/library/io.html#overview) \t|\n",
        ">[matplotlib.pyplot](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.html) \t|\t [ipywidgets](https://github.com/jupyter-widgets/ipywidgets)\t|\t[seaborn](https://seaborn.pydata.org/) \t|\n",
        "> [pysal](https://pysal.readthedocs.io/en/v1.11.0/users/introduction.html) \t|\t [os](https://docs.python.org/3/library/os.html)\t|\t[gdal](https://gdal.org/)\t|\n",
        ">  [pkg_resources](https://setuptools.readthedocs.io/en/latest/pkg_resources.html#overview)\t|\t[whitebox](https://jblindsay.github.io/wbt_book/intro.html) \t|\t[imageio](https://pypi.org/project/imageio/) \t|\n",
        "> [IPython](https://ipython.org/)\t|\t[imageio tifffile](https://pypi.org/project/tifffile/)\t|\t\t|\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        " plugins| |\n",
        "--- |--- |\n",
        "HeatMapWithTime \n",
        "HeatMap\n",
        "MeasureControl\n",
        "PrepareUrBasemaps_CreateLayers from [folium.plugins]\n",
        "cluster (from sklearn)\n",
        "rasterio.plot\n",
        "mapping (from shapely.geometry)\n",
        "Point(from shapely.geometry)  \n",
        "mask (from rasterio.mask) \n",
        "Moran  (from pysal.explore.esda.moran) \n",
        "Moran_Local (from pysal.explore.esda.moran) \n",
        "\n",
        "<br>\n",
        "\n",
        "At this point you have learned so many thigs that the lexicode is non-exhaustive. It might be useful to make your own custum 'lexicode' with the commands you use the most often.\n",
        "\n",
        "<br>\n",
        "\n",
        ">Lexicode_MakingaBasicMap \t|\t Lexicode_Webmaps&Distributions \t|\tLexicode_StreetGridOrientations \t|\t Lexicode_SpatialPatterns \t|\t Lexicode_RasterLandscape \t|\t Lexicode_Pysal_weights_Moran_Lisa\t|\n",
        ">--- \t|\t --- \t|\t ---\t|\t---\t|\t----\t|\t---\t|\n",
        "> ==   () [] \t|\t pd.concat() \t|\t { } *subselection from list*\t|\t%matplotlib inline  \t|\t .open()\t|\t.tail()\t|\n",
        ">.head_csv() \t|\t .dtype() \t|\t ox.gdf_from_places()\t|\trequests.get()\t|\t.print()\t|\tax.set_axis_off()\t|\n",
        ">.read_csv() \t|\t astype() \t|\t ox.plot_shape()\t|\trequest.content()\t|\tdataset.name\t|\tf.set_facecolor\t|\n",
        ">mean()  \t|\t fillna()\t|\tnetwork_type= ''\t|\t.bytes()\t|\tdataset.count\t|\tplt.axis\t|\n",
        ">folium.Map \t|\t def return \t|\tox.add_edge_bearings(ox.get_undirected())\t|\tgpd.GeoDataFrame.from_features()\t|\tdataset.shape\t|\tf.suptitle()\t|\n",
        ">range() \t|\t .apply(lambda x:*function*,axis=) \t|\tcount_and_merge()\t|\tSet()\t|\tdataset.descriptions\t|\tf.set_facecolor\t|\n",
        ">len() \t|\t pd.merge() \t|\tnp.arrange()\t|\tpd.value_counts() \t|\tdataset.meta\t|\tplt.axis\t|\n",
        ">iloc[]\t|\t how= , left_index= ,left_index= \t|\tnp.histogram()\t|\t.merge()\t|\tdataset.driver\t|\tf.suptitle()\t|\n",
        ">.value_counts()\t|\t gpd.GeoDataFrame()\t|\t ax.set_theta_location()\t|\t.sort_values\t|\tdataset.read\t|\t.remap_ids\t|\n",
        ">if =:\t|\t geometry=gpd.points_from_xy \t|\tax.set_ylim()\t|\tcluster.KMeans()\t|\t.shape\t|\t.transform\t|\n",
        ">elif =: \t|\tprint() \t|\tax.set_title()\t|\t.fit()\t|\tnp.amean()\t|\tlag_spatial()\t|\n",
        ">else =:\t|\t .isin()\t|\tax.set_yticks()\t|\t.drop() \t|\tnp.amin ()\t|\t.mean()\t|\n",
        ">folium.Marker()\t|\t classic.plot()\t|\tax.set_xlabels() & ax.set_yticklabels\t|\t.assign()\t|\tnp.amax()\t|\t.std()\t|\n",
        ">folium.Icon()\t|\t generateBaseMap()\t|\tplt.subplots()\t|\tplt.show()\t|\tnp.std()\t|\tsns.regplot()\t|\n",
        ">folium.Circle\t|\t .groupby(['', ''])\t|\t.dropna()\t|\t.set_title\t|\tshow()\t|\tMoran_local\t|\n",
        ">popup= \t|\t .reset_index() \t|\tpolar_plot()\t|\tsns.pairplot()\t|\tcmap=\t|\tp.sim()\t|\n",
        ">radius= \t|\t  max_zoom= \t|\tpd.Series()\t|\tarrs.append()\t|\tnp.seterr()\t|\tLisa.q\t|\n",
        ">.values.tolist() \t|\tfolium.TileLayer()\t|\tnp.pi\t|\t.show_hist()\t|\t[plt.imshow](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.imshow.html)\t|\twbt.set_working_dir()\t|\n",
        ">.add_to()\t|\t plugins.DualMap(location= , tiles= , zoom_start= )\t|\t \t|\t \t|\tclipped_img\t|\twbt.verbose()\t|\n",
        "> \t|\t \t|\t \t|\t \t|\trd.TerrainAttribute()\t|\t.to_file(\" \")\t|\n",
        ">  \t|\t \t|\t \t|\t \t|\trdShow() \t|\twbt.idw_interpolation()\t|\n",
        ">  \t|\t \t|\t \t|\t \t|\tgdal_data.GetRasterBand()\t|\timageio.imread(os.path.join())\t|\n",
        ">  \t|\t \t|\t \t|\t \t|\tgdal_band.GetNoDataValue()\t|\twbt.ks_test_for_normality()\t|\n",
        ">  \t|\t \t|\t \t|\t \t|\tgdal_data.ReadAsArray\t|\tIPython.display.HTML()\t|\n",
        ">  \t|\t \t|\t \t|\t \t|\tgdal_data.ReadAsArray().astype(np.float)\t|\twbt.image_autocorrelation()\t|\n",
        ">  \t|\t \t|\t \t|\t \t|\tplt.contour()\t|\t\t|\n",
        ">  \t|\t \t|\t \t|\t \t|\tplt.contourf()\t|\t\t|\n",
        "\n",
        "\n"
      ]
    }
  ]
}